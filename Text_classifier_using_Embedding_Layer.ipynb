{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gZihzubKfEu"
      },
      "outputs": [],
      "source": [
        "#Text classifier using Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xCf_afsnLvJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To unzip and read the csv file inside the zip file\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/BBC News Train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('extracted_data')"
      ],
      "metadata": {
        "id": "P7Qls1qAMDv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"extracted_data/BBC News Train.csv\", 'r') as csvfile:\n",
        "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"The second line (first data point) looks like this:\\n\\n{csvfile.readline()}\")"
      ],
      "metadata": {
        "id": "2JW2X0jlMJr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining useful global variables"
      ],
      "metadata": {
        "id": "YKTmkaWlMWQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VOCAB_SIZE: The maximum number of words to keep, based on word frequency. Defaults to 1000.\n",
        "\n",
        "EMBEDDING_DIM: Dimension of the dense embedding, will be used in the embedding layer of the model. Defaults to 16.\n",
        "\n",
        "MAX_LENGTH: Maximum length of all sequences. Defaults to 120.\n",
        "\n",
        "TRAINING_SPLIT: Proportion of data used for training. Defaults to 0.8"
      ],
      "metadata": {
        "id": "ZCe1h5LeMr7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the global variables\n",
        "VOCAB_SIZE =\n",
        "EMBEDDING_DIM =\n",
        "MAX_LENGTH =\n",
        "TRAINING_SPLIT ="
      ],
      "metadata": {
        "id": "RDyxF7KkMa4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and pre-processing the data"
      ],
      "metadata": {
        "id": "UwRf-s2ZOB5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = # provide the data directory\n",
        "data = np.loadtxt(data_dir, delimiter=',', skiprows=1, dtype='str', comments=None)\n",
        "print(f\"Shape of the data: {data.shape}\")\n",
        "print(f\"{data[0]}\\n{data[1]}\")"
      ],
      "metadata": {
        "id": "ghRw-Iu1OG29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "print(f\"There are {len(data)} sentence-label pairs in the dataset.\\n\")\n",
        "print(f\"First sentence has {len((data[0,1]).split())} words.\\n\")\n",
        "print(f\"The first 5 labels are {data[:5,2]}\")"
      ],
      "metadata": {
        "id": "y4mHSHyVOazd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "polished-eagle"
      },
      "source": [
        "## Training - Validation Datasets\n",
        "\n",
        "Now you will code the `train_val_datasets` function, which, given the `data` DataFrame, should return the training and validation datasets, consisting of `(text, label)` pairs. For this last part, you will be using the [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_val_datasets\n",
        "def train_val_datasets(data,train_split=0.8):\n",
        "    '''\n",
        "    Splits data into traning and validations sets\n",
        "\n",
        "    Args:\n",
        "        data (np.array): array with two columns, first one is the label, the second is the text\n",
        "\n",
        "    Returns:\n",
        "        (tf.data.Dataset, tf.data.Dataset): tuple containing the train and validation datasets\n",
        "    '''\n",
        "   ### START CODE HERE ###\n",
        "\n",
        "    # Compute the number of samples that will be used for training\n",
        "    train_size =\n",
        "\n",
        "    # Slice the dataset to get only the texts and labels\n",
        "    texts =\n",
        "    labels =\n",
        "\n",
        "    # Split the texts and labels into train/validation splits\n",
        "    train_texts =\n",
        "    validation_texts =\n",
        "    train_labels =\n",
        "    validation_labels =\n",
        "\n",
        "    # Create the train and validation datasets from the splits\n",
        "    train_dataset =\n",
        "    validation_dataset =\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    return train_dataset, validation_dataset"
      ],
      "metadata": {
        "id": "ablWQ69vOv7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the datasets\n",
        "train_dataset, validation_dataset = train_val_datasets(data)\n",
        "print('Name:        Register Number:       ')\n",
        "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\\n\")\n",
        "print(f\"There are {validation_dataset.cardinality()} sentence-label pairs for validation.\\n\")"
      ],
      "metadata": {
        "id": "deuClmbFObsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization - Sequences and padding"
      ],
      "metadata": {
        "id": "0NP8gNmaP65c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_func(sentence):\n",
        "    \"\"\"\n",
        "    Removes a list of stopwords\n",
        "\n",
        "    Args:\n",
        "        sentence (tf.string): sentence to remove the stopwords from\n",
        "\n",
        "    Returns:\n",
        "        sentence (tf.string): lowercase sentence without the stopwords\n",
        "    \"\"\"\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\",  \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",  \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\",  \"should\", \"so\", \"some\", \"such\", \"than\", \"that\",  \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\",  \"were\", \"what\",  \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"why\", \"with\", \"would\", \"you\",  \"your\", \"yours\", \"yourself\", \"yourselves\", \"'m\",  \"'d\", \"'ll\", \"'re\", \"'ve\", \"'s\", \"'d\"]\n",
        "\n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "\n",
        "    # Remove stopwords\n",
        "    for word in stopwords:\n",
        "        if word[0] == \"'\":\n",
        "            sentence = tf.strings.regex_replace(sentence, rf\"{word}\\b\", \"\")\n",
        "        else:\n",
        "            sentence = tf.strings.regex_replace(sentence, rf\"\\b{word}\\b\", \"\")\n",
        "\n",
        "    # Remove punctuation\n",
        "    sentence = tf.strings.regex_replace(sentence, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
        "\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "H6iPrw5tP_PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. fit_vectorizer\n",
        "Next complete the fit_vectorizer function below. This function should return a TextVectorization layer that has already been fitted on the training sentences. The vocabulary learned by the vectorizer should have VOCAB_SIZE size, and truncate the output sequences to have MAX_LENGTH length.\n",
        "\n",
        "Remember to use the custom function standardize_func to standardize each sentence in the vectorizer. You can do this by passing the function to the standardize parameter of TextVectorization."
      ],
      "metadata": {
        "id": "Exs2-mJWQOP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "lines_to_next_cell": 2,
        "tags": [
          "graded"
        ],
        "id": "recreational-prince"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: fit_vectorizer\n",
        "def fit_vectorizer(train_sentences, standardize_func):\n",
        "    '''\n",
        "    Defines and adapts the text vectorizer\n",
        "\n",
        "    Args:\n",
        "        train_sentences (tf.data.Dataset): sentences from the train dataset to fit the TextVectorization layer\n",
        "        standardize_func (FunctionType): function to remove stopwords and punctuation, and lowercase texts.\n",
        "    Returns:\n",
        "        TextVectorization: adapted instance of TextVectorization layer\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # If train_sentences is a NumPy array, convert it to a TensorFlow Dataset\n",
        "\n",
        "\n",
        "    # Initialize the TextVectorization layer\n",
        "    vectorizer =\n",
        "\n",
        "\n",
        "    # Adapt the vectorizer to the training sentences\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vectorizer\n",
        "text_only_dataset = train_dataset.map(lambda text, label: text)\n",
        "vectorizer = fit_vectorizer(text_only_dataset, standardize_func)\n",
        "vocab_size = vectorizer.vocabulary_size()\n",
        "print('Name:        Register Number:       ')\n",
        "print(f\"Vocabulary contains {vocab_size} words\\n\")"
      ],
      "metadata": {
        "id": "ND496KNuPrkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H25lRAUWRnXl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "familiar-reform"
      },
      "source": [
        "### 3. fit_label_encoder\n",
        "\n",
        "Remember your categories are also text labels, so you need to encode the labels as well. For this complete the `tokenize_labels` function below.\n",
        "\n",
        "- Use the function [`tf.keras.layers.StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) to encode the labels. Use the correct parameters so that you don't include any OOV tokens.\n",
        "- You should fit the tokenizer to all the labels to avoid the case of a particular label not being present in the validation set. Since you are dealing with labels there should never be an OOV label. For this, you can concatenate the two datasets using the [`concatenate`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#concatenate) method from `tf.data.Dataset` objects."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: fit_label_encoder\n",
        "def fit_label_encoder(train_labels, validation_labels):\n",
        "    \"\"\"Creates an instance of a StringLookup, and trains it on all labels\n",
        "\n",
        "    Args:\n",
        "        train_labels (tf.data.Dataset): dataset of train labels\n",
        "        validation_labels (tf.data.Dataset): dataset of validation labels\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.layers.StringLookup: adapted encoder for train and validation labels\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Join the two label datasets by concatenating them\n",
        "\n",
        "\n",
        "    # Instantiate the StringLookup layer. We set mask_token=None and num_oov_indices=0 to avoid OOV tokens\n",
        "\n",
        "\n",
        "    # Fit the StringLookup layer on the concatenated labels\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return label_encoder"
      ],
      "metadata": {
        "id": "BBTNcNKwRxQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the label encoder\n",
        "train_labels_only = train_dataset.map(lambda text, label: label)\n",
        "validation_labels_only = validation_dataset.map(lambda text, label: label)\n",
        "\n",
        "label_encoder = fit_label_encoder(train_labels_only,validation_labels_only)\n",
        "print('Name:        Register Number:       ')\n",
        "print(f'Unique labels: {label_encoder.get_vocabulary()}')"
      ],
      "metadata": {
        "id": "NzAx1s0BSF3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iL8Wb4uASMsF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sweet-sentence"
      },
      "source": [
        "### 4. preprocess_dataset\n",
        "\n",
        "Now that you have trained the vectorizer for the texts and the encoder for the labels, it's time for you to actually transform the dataset. For this complete the `preprocess_dataset` function below.\n",
        "Use this function to set the dataset batch size to 32\n",
        "\n",
        "- You can apply the preprocessing to each pair or text and label by using the [`.map`]method.\n",
        "- You can set the batchsize to any Dataset by using the [`.batch`] method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: preprocess_dataset\n",
        "def preprocess_dataset(dataset, text_vectorizer, label_encoder):\n",
        "    \"\"\"Apply the preprocessing to a dataset\n",
        "\n",
        "    Args:\n",
        "        dataset (tf.data.Dataset): dataset to preprocess\n",
        "        text_vectorizer (tf.keras.layers.TextVectorization ): text vectorizer\n",
        "        label_encoder (tf.keras.layers.StringLookup): label encoder\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: transformed dataset\n",
        "    \"\"\"\n",
        "\n",
        "      ### START CODE HERE ###\n",
        "\n",
        "    # Apply text vectorization and label encoding\n",
        "\n",
        "    # Set the batch size to 32\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "cZGLqYQGSf4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess your dataset\n",
        "train_proc_dataset = preprocess_dataset(train_dataset, vectorizer, label_encoder)\n",
        "validation_proc_dataset = preprocess_dataset(validation_dataset, vectorizer, label_encoder)"
      ],
      "metadata": {
        "id": "ACLhQOe_SoM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = next(train_proc_dataset.as_numpy_iterator())\n",
        "validation_batch = next(validation_proc_dataset.as_numpy_iterator())\n",
        "print('Name:        Register Number:       ')\n",
        "print(f\"Shape of the train batch: {train_batch[0].shape}\")\n",
        "print(f\"Shape of the validation batch: {validation_batch[0].shape}\")"
      ],
      "metadata": {
        "id": "gt1skObIStCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5: create_model\n",
        "Define the model that will classify each text as being part of a certain category.\n",
        "\n",
        "The last layer should be a Dense layer with 5 units (since there are 5 categories) with a softmax activation.\n",
        "You should also compile your model using an appropriate loss function and optimizer.\n",
        "You can use any architecture you want but keep in mind that this problem doesn't need many layers to be solved successfully. You don't need any layers beside Embedding, GlobalAveragePooling1D and Dense layers but feel free to try out different architectures."
      ],
      "metadata": {
        "id": "sMYUI6M_S9fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "    \"\"\"\n",
        "    Creates a text classifier model\n",
        "    Returns:\n",
        "      tf.keras Model: the text classifier model\n",
        "    \"\"\"\n",
        "\n",
        "      ### START CODE HERE ###\n",
        "\n",
        "    model =\n",
        "\n",
        "\n",
        "    # Compile the model with appropriate loss, optimizer, and metrics\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XNJUFyvuSvWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the untrained model\n",
        "model ="
      ],
      "metadata": {
        "id": "7fqibq2oTzCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = train_proc_dataset.take(1)\n",
        "\n",
        "try:\n",
        "\tmodel.evaluate(example_batch, verbose=False)\n",
        "except:\n",
        "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
        "else:\n",
        "\tpredictions = model.predict(example_batch, verbose=False)\n",
        "\tprint(f\"predictions have shape: {predictions.shape}\")"
      ],
      "metadata": {
        "id": "Ey5NzSWKT1pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_proc_dataset, epochs=30, validation_data=validation_proc_dataset)"
      ],
      "metadata": {
        "id": "2xrjD-XgUDDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[f'val_{metric}'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, f'val_{metric}'])\n",
        "    plt.show()\n",
        "print('Name:        Register Number:       ')\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "7NKZZ19xUKtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}